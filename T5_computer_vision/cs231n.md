## What is L1, L2?

### L1(Manhattan distance) 


$$
abs(l1 - l2)
$$

**Loss**
- Robust <- outlier 영향 덜받음
- unstable
- multiple solution <- manhattan 특성(이동 방법 많음)

<br>

### L2(Euclidean distance) 

$$
(l1 - l2) ** 2
$$

**Loss**
- Not Robust
- stable
- one solution 

<br>

## SVM vs softMax

### Multiclass SVM loss

<img src="./image/svm.png" width = "400">

- sj : 오답클래스 예측 점수, syi : 정답클래스 예측 점수, +1 : safety margin
- (sj + 1) - syi로 식을 다시 보면, 정답클래스의 예측 점수가 다른 클래스보다 margin만큼 커야 loss가 발생 하지 않음


### SVM vs softMax

- SVM은 분류 가능한 margin 값을 찾기만 하면 끝
- softMax는 확률 질량 1 안에서 올바른 클래스의 질량을 계속해서 늘리기를 원함


<br>


## Zero-Centering

### why zero centering?

input 값이 모두 positive 이거나 negative라면 weight update 불가

-> **sigmoid function** 에서 input 값이 항상 양수

-> 한 쪽 방향으로만 weight update 될 것


### image는 zero center만

- image는 어느 정도 각 차원간에 스케일이 맞춰서 있어 normalize보단 zero center정도만